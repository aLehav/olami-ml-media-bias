{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (STEP 1) Count Phrase Occurances\n",
    "\n",
    "## NEED: method to determine which phrases to keep track of\n",
    "\n",
    "## METHOD: develop and create a frequency matrix with all of our articles\n",
    "## and tracking the frequency of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m frequency_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(articles)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get the feature names that match your phrases\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m matched_phrases \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mphrases\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with only the columns for your phrases\u001b[39;00m\n\u001b[0;32m     22\u001b[0m frequency_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(frequency_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m frequency_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(articles)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get the feature names that match your phrases\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m matched_phrases \u001b[38;5;241m=\u001b[39m [phrase \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m phrases \u001b[38;5;28;01mif\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with only the columns for your phrases\u001b[39;00m\n\u001b[0;32m     22\u001b[0m frequency_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(frequency_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1487\u001b[0m, in \u001b[0;36mCountVectorizer.get_feature_names_out\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m-> 1487\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1489\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;124;03m    Transformed feature names.\u001b[39;00m\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m-> 1487\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))],\n\u001b[0;32m   1488\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[0;32m   1489\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load articles\n",
    "articles_df = pd.read_csv('10k_articles_formatted.csv')\n",
    "articles = articles_df['content'].astype(str).apply(lambda x: x.lower().replace('period', '.'))\n",
    "\n",
    "# Load phrases\n",
    "phrases_df = pd.read_csv('Gensim_Python_Notebook_Results\\\\remaining_phrases.csv')\n",
    "phrases = phrases_df['phrase'].tolist()\n",
    "\n",
    "# Initialize CountVectorizer with n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Fit the vectorizer to the articles and transform the data\n",
    "frequency_matrix = vectorizer.fit_transform(articles)\n",
    "\n",
    "# Get the feature names that match your phrases\n",
    "matched_phrases = [phrase for phrase in phrases if phrase in vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Create a DataFrame with only the columns for your phrases\n",
    "frequency_df = pd.DataFrame(frequency_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "frequency_df = frequency_df[matched_phrases]\n",
    "\n",
    "# Optional: Set newspaper names as the index\n",
    "frequency_df.index = articles_df['name']\n",
    "\n",
    "frequency_df.to_csv('frequency_matrix.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the first few rows of the frequency matrix\n",
    "print(frequency_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    tennis  restaurants  world series  \\\n",
      "name                                                                    \n",
      "SUNY Stony Brook                         0            0             0   \n",
      "Capital University                       0            0             0   \n",
      "University of Pittsburgh                 0            0             0   \n",
      "University of Missouri                   0            1             0   \n",
      "University of Utah                       0            0             0   \n",
      "...                                    ...          ...           ...   \n",
      "Pennsylvania State University            0            0             0   \n",
      "Eastern Illinois University              0            0             0   \n",
      "University of Oklahoma                   0            0             0   \n",
      "University of California San Diego       0            0             0   \n",
      "Washington State University              0            0             0   \n",
      "\n",
      "                                    swimming  tax evasion  veterans  brain  \\\n",
      "name                                                                         \n",
      "SUNY Stony Brook                           0            0         0      0   \n",
      "Capital University                         0            0         0      0   \n",
      "University of Pittsburgh                   0            0         0      0   \n",
      "University of Missouri                     0            0         0      0   \n",
      "University of Utah                         0            0         0      0   \n",
      "...                                      ...          ...       ...    ...   \n",
      "Pennsylvania State University              0            0         0      0   \n",
      "Eastern Illinois University                0            0         0      0   \n",
      "University of Oklahoma                     0            0         0      0   \n",
      "University of California San Diego         0            0         0      0   \n",
      "Washington State University                0            0         0      0   \n",
      "\n",
      "                                    dairy products  light  food  \n",
      "name                                                             \n",
      "SUNY Stony Brook                                 0      0     0  \n",
      "Capital University                               0      0     0  \n",
      "University of Pittsburgh                         0      0     0  \n",
      "University of Missouri                           0      0     0  \n",
      "University of Utah                               0      0     0  \n",
      "...                                            ...    ...   ...  \n",
      "Pennsylvania State University                    0      0     0  \n",
      "Eastern Illinois University                      0      0     0  \n",
      "University of Oklahoma                           0      0     0  \n",
      "University of California San Diego               0      0     0  \n",
      "Washington State University                      0      0     0  \n",
      "\n",
      "[100 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#### TESTING SCRIPT\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming articles_df and phrases_df are loaded from somewhere before this point\n",
    "\n",
    "# Sample a subset of the data for testing\n",
    "sample_articles_df = articles_df.sample(n=200, random_state=42) # Ensure reproducibility with a fixed random state\n",
    "sample_phrases_df = phrases_df.sample(n=50, random_state=42) \n",
    "phrases = sample_phrases_df['phrase'].tolist()\n",
    "\n",
    "# Process articles\n",
    "sample_articles = sample_articles_df['content'].astype(str).apply(lambda x: x.lower().replace('period', '.'))\n",
    "\n",
    "# Initialize CountVectorizer with n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Fit the vectorizer to the sample articles and transform the data\n",
    "frequency_matrix = vectorizer.fit_transform(sample_articles)\n",
    "\n",
    "# Get the feature names that match your phrases\n",
    "matched_phrases = [phrase for phrase in phrases if phrase in vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Create a DataFrame with only the columns for your phrases\n",
    "frequency_df = pd.DataFrame(frequency_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "frequency_df = frequency_df[matched_phrases]\n",
    "\n",
    "# Optional: Set newspaper names as the index from the sample\n",
    "frequency_df.index = sample_articles_df['name']\n",
    "\n",
    "# Save the test frequency matrix to a CSV file\n",
    "frequency_df.to_csv('test_frequency_matrix.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the frequency matrix\n",
    "print(frequency_df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (STEP 2) Use Poisson Distribution for Analysis\n",
    "## Use a Poisson Distribution to accurately keep track of our\n",
    "## frequency data (Nij)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Variables\n",
    "num_articles = 10000  \n",
    "num_keywords = 366    \n",
    "lambda_per_article = 0.5  # Average rate (λ) of keyword occurrence per article, to be adjusted\n",
    "\n",
    "# Simulate occurrences of each keyword in each article\n",
    "# AForsimplicity, we assume each keyword has the same λ across all articles\n",
    "keyword_occurrences = np.random.poisson(lambda_per_article, (num_articles, num_keywords))\n",
    "\n",
    "print(f\"Shape of occurrences matrix: {keyword_occurrences.shape}\")\n",
    "print(f\"Total occurrences of the first keyword across all articles: {keyword_occurrences[:, 0].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (STEP 3) SVD application and (STEP 4) Modifying the SVD to address\n",
    "# shortcomings relating to the specific nature of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (STEP 5) Data tranformation using the Poisson Likelihood function\n",
    "# (the log of) to best explain observed phrase counts\n",
    "\n",
    "## PURPOSE: Adjusts the model to enable the predicted phrase counts to closely\n",
    "## mirror the actual phrase counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (STEP 6) -> Gradient Descent for data optimization\n",
    "# (STEP 7) -> Getting rid of negative counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
